{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "common-luxury",
   "metadata": {},
   "source": [
    "##  Contents<a id='Contents'></a>\n",
    "* [Introduction](#Introduction)\n",
    "  * [Imports](#Imports)\n",
    "  * [Load The Data](#Load_The_Data)\n",
    "  * [Predicting](#Predicting)\n",
    "      * [Multinomial Naive Bayes](#_Multinomial_Naive_Bayes)\n",
    "      * [Random Forest](#_Random_Forest)\n",
    "      * [Multinomial Naive Bayes for Resumes](#_Multinomial_Naive_Bayes_for_Resumes)\n",
    "  * [Tokenization and Lemmatization](#_Tokenization_and_Lemmatization)\n",
    "  * [Count Vectorizer](#_Count_Vectorizer_)\n",
    "  * [Most Common Words](#_Most_Common_Words_)\n",
    "  * [Least Common Words](#_Least_Common_Words_)\n",
    "  * [Pentagrams](#Pentagrams)\n",
    "  * [Contractions and Spacy](#Contractions_and_Spacy)\n",
    "  * [Removing Stopwords and Lemmatization with Spacy](#Removing_Stopwords_and_Lemmatization_Spacy)\n",
    "  * [Document Term Matrices and Sentiment Analysis](#Document_Term_Matrices_and_Sentiment_Analysis)\n",
    "  * [Summary](#_Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-approval",
   "metadata": {},
   "source": [
    "## Introduction<a id='Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-hybrid",
   "metadata": {},
   "source": [
    "Preprocessing is an often overlooked step in NLP problems.\n",
    "\n",
    "To preprocess your text means to bring your text into a form that is predictable and analyzable for your task. A task here is a combination of approach and domain. \n",
    "\n",
    "Task = approach + domain\n",
    "\n",
    "Text preprocessing is not directly transferable from task to task! Like all data science projects, a well defined goal for the project must be declared at the beginning of the task. This goal informs all of the steps in the data science method, including preprocessing.\n",
    "\n",
    "A lot of the usual NLP preprocessing techniques (lowercasing, lemmatization, stemming, stop word removal, etc.) was performed in the wrangling/EDA step in this project. These techniques will be applied in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-devil",
   "metadata": {},
   "source": [
    "## Imports<a id='Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "soviet-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib as plty\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compact-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stop = set(stopwords.words('english'))\n",
    "from sklearn import preprocessing\n",
    "Encode = preprocessing.LabelEncoder()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB = MultinomialNB()\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "minimal-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\daenj\\OneDrive\\Desktop\\Datasets\\Capstone 2 2021')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-living",
   "metadata": {},
   "source": [
    "## Loading the Data<a id='Loading the Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-packaging",
   "metadata": {},
   "source": [
    "Due to the size and well-written nature of both data sets, only light to no preprocessing is required. A domain specific dataset with sparse data (i.e tweets about a product) would require noise removal, lowercasing, stemming, etc. Basically, lot's of preprocessing layers would be required in this case, this situation is not one of those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "every-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Sheet_1.csv', encoding='latin-1')\n",
    "\n",
    "data.drop([\"Unnamed: 3\",\"Unnamed: 4\",\"Unnamed: 5\",\"Unnamed: 6\",\"Unnamed: 7\"], axis = 1, inplace =True)\n",
    "\n",
    "data = pd.concat([data[\"class\"],data[\"response_text\"]], axis = 1)\n",
    "\n",
    "data.dropna(axis=0, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "numeric-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('Sheet_2.csv', encoding='latin-1')\n",
    "\n",
    "data2.dropna(axis=0, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "revised-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"class\"] = [1 if each == \"flagged\" else 0 for each in data[\"class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vanilla-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[\"class\"] = [1 if each == \"flagged\" else 0 for each in data2[\"class\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-forest",
   "metadata": {},
   "source": [
    "The above code changes the 'class' column of both datasets into binary values, 1 for 'flagged', 0 for 'not flagged'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-proposition",
   "metadata": {},
   "source": [
    "## Predicting<a id='Predicting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-style",
   "metadata": {},
   "source": [
    "Scikit-learn’s CountVectorizer is used to convert a collection of text documents to a vector of term/token counts. It also enables the preprocessing of text data prior to generating the vector representation. Countvectorizer is the only preprocessing used on the text data in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-cooperation",
   "metadata": {},
   "source": [
    "The following cells use a train/test split of 75/25 on both datasets. The first model used is a multinomial Naive Bayes classifier. The multinomial Naive Bayes classifier is suitable for classification with discrete features (i.e, word counts for text classification). The multinomial distribution normally requires integer feature counts, which comes via the count vectorizer. The second model used is a random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Let's look at various metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-watch",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes<a id='Multinomial Naive Bayes'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cosmetic-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data.response_text\n",
    "y = data['class']\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.25, random_state=42)\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)\n",
    "NB.fit(x_train_dtm,y_train)\n",
    "y_predict = NB.predict(x_test_dtm)\n",
    "metrics.accuracy_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "secondary-simon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aging-fruit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46153846153846156"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-cylinder",
   "metadata": {},
   "source": [
    "## Random Forest<a id='Random Forest'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cathedral-geometry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_depth=10,max_features=10)\n",
    "rf.fit(x_train_dtm,y_train)\n",
    "rf_predict = rf.predict(x_test_dtm)\n",
    "metrics.accuracy_score(y_test,rf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-phenomenon",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes for Resumes<a id='Multinomial Naive Bayes for Resumes'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "interesting-clarity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data2.resume_text\n",
    "y = data2['class']\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.25, random_state=42)\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)\n",
    "NB.fit(x_train_dtm,y_train)\n",
    "y_predict = NB.predict(x_test_dtm)\n",
    "metrics.accuracy_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "binary-orleans",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pointed-season",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39999999999999997"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-manufacturer",
   "metadata": {},
   "source": [
    "## Random Forest for Resumes<a id='Random Forest for Resumes'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "official-blame",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65625"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_depth=10,max_features=10)\n",
    "rf.fit(x_train_dtm,y_train)\n",
    "rf_predict = rf.predict(x_test_dtm)\n",
    "metrics.accuracy_score(y_test,rf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-lodging",
   "metadata": {},
   "source": [
    "The precision and f1_scores for the random forest models was 0, so these metrics will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-separation",
   "metadata": {},
   "source": [
    "## Preprocessing<a id='Preprocessing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-battery",
   "metadata": {},
   "source": [
    "In this next section, various layers of preprocessing will be applied to the data. These include lowercasing, tokenization, lemmatization, and count vectorization. A quick note, using the function below and the resulting sparse matrix in the train/test split only works for the chatbot response data. The Gaussian Naive Bayes classifier will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "conservative-nursing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\daenj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk as nlp\n",
    "nltk.download(\"stopwords\")    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "allied-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_list = []\n",
    "for description in data.response_text:\n",
    "    \n",
    "    \n",
    "    description = re.sub(\"[^a-zA-Z]\",\" \",description)\n",
    "    description = description.lower() \n",
    "    \n",
    "    description = nltk.word_tokenize(description)\n",
    "    description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    description = (lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, \"n\"),pos = \"v\"),pos=\"a\") for word in description)\n",
    "    \n",
    "    description = \" \".join(description)\n",
    "    description_list.append(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "experimental-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "macro-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "wrong-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparce_matrix = count_vectorizer.fit_transform(description_list).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "impressed-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['class']\n",
    "x = sparce_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "norman-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "linear-handbook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "introductory-strength",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb.predict(x_test)\n",
    "print(\"Accuracy:\",nb.score(y_pred.reshape(-1,1),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-insight",
   "metadata": {},
   "source": [
    "The accuracy score for both the GaussianNB and Random Forest classifiers are the same! The GaussianNB classifier performs better on the chatbot response data than the MultinomialNB classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-oliver",
   "metadata": {},
   "source": [
    "## More Preprocessing<a id='More Preprocessing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-franchise",
   "metadata": {},
   "source": [
    "The following section won't be used in a predictive model, it's purpose is to show some of the NLP preprocessing techniques that are available to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hazardous-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# init stemmer\n",
    "porter_stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tropical-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem raw words with noise\n",
    "raw_words=data['response_text']\n",
    "stemmed_words=[porter_stemmer.stem(word=word) for word in raw_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "collectible-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem raw words with noise\n",
    "raw_words2=data2['resume_text']\n",
    "stemmed_words=[porter_stemmer.stem(word=word) for word in raw_words2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "behind-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    \"\"\"Basic cleaning of texts.\"\"\"\n",
    "    \n",
    "    # remove html markup\n",
    "    text=re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "    #remove non-ascii and digits\n",
    "    text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
    "    \n",
    "    #remove whitespace\n",
    "    text=text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "interracial-trinity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_word</th>\n",
       "      <th>cleaned_word</th>\n",
       "      <th>stemmed_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I try and avoid this sort of conflict</td>\n",
       "      <td>I try and avoid this sort of conflict</td>\n",
       "      <td>i try and avoid this sort of conflict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Had a friend open up to me about his mental ad...</td>\n",
       "      <td>Had a friend open up to me about his mental ad...</td>\n",
       "      <td>had a friend open up to me about his mental ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I saved a girl from suicide once. She was goin...</td>\n",
       "      <td>I saved a girl from suicide once  She was goin...</td>\n",
       "      <td>i saved a girl from suicide once  she was goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i cant think of one really...i think i may hav...</td>\n",
       "      <td>i cant think of one really   i think i may hav...</td>\n",
       "      <td>i cant think of one really   i think i may hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Only really one friend who doesn't fit into th...</td>\n",
       "      <td>Only really one friend who doesn t fit into th...</td>\n",
       "      <td>only really one friend who doesn t fit into th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Now that I've been through it, although i'm no...</td>\n",
       "      <td>Now that I ve been through it  although i m no...</td>\n",
       "      <td>now that i ve been through it  although i m no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>when my best friends mom past away from od'ing...</td>\n",
       "      <td>when my best friends mom past away from od ing...</td>\n",
       "      <td>when my best friends mom past away from od ing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>As a camp counselor I provide stability in kid...</td>\n",
       "      <td>As a camp counselor I provide stability in kid...</td>\n",
       "      <td>as a camp counselor i provide stability in kid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>My now girlfriend used to have serious addicti...</td>\n",
       "      <td>My now girlfriend used to have serious addicti...</td>\n",
       "      <td>my now girlfriend used to have serious addicti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>The one person I ever talked to it was because...</td>\n",
       "      <td>The one person I ever talked to it was because...</td>\n",
       "      <td>the one person i ever talked to it was because...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_word  \\\n",
       "0               I try and avoid this sort of conflict   \n",
       "1   Had a friend open up to me about his mental ad...   \n",
       "2   I saved a girl from suicide once. She was goin...   \n",
       "3   i cant think of one really...i think i may hav...   \n",
       "4   Only really one friend who doesn't fit into th...   \n",
       "..                                                ...   \n",
       "75  Now that I've been through it, although i'm no...   \n",
       "76  when my best friends mom past away from od'ing...   \n",
       "77  As a camp counselor I provide stability in kid...   \n",
       "78  My now girlfriend used to have serious addicti...   \n",
       "79  The one person I ever talked to it was because...   \n",
       "\n",
       "                                         cleaned_word  \\\n",
       "0               I try and avoid this sort of conflict   \n",
       "1   Had a friend open up to me about his mental ad...   \n",
       "2   I saved a girl from suicide once  She was goin...   \n",
       "3   i cant think of one really   i think i may hav...   \n",
       "4   Only really one friend who doesn t fit into th...   \n",
       "..                                                ...   \n",
       "75  Now that I ve been through it  although i m no...   \n",
       "76  when my best friends mom past away from od ing...   \n",
       "77  As a camp counselor I provide stability in kid...   \n",
       "78  My now girlfriend used to have serious addicti...   \n",
       "79  The one person I ever talked to it was because...   \n",
       "\n",
       "                                         stemmed_word  \n",
       "0               i try and avoid this sort of conflict  \n",
       "1   had a friend open up to me about his mental ad...  \n",
       "2   i saved a girl from suicide once  she was goin...  \n",
       "3   i cant think of one really   i think i may hav...  \n",
       "4   only really one friend who doesn t fit into th...  \n",
       "..                                                ...  \n",
       "75  now that i ve been through it  although i m no...  \n",
       "76  when my best friends mom past away from od ing...  \n",
       "77  as a camp counselor i provide stability in kid...  \n",
       "78  my now girlfriend used to have serious addicti...  \n",
       "79  the one person i ever talked to it was because...  \n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stem words already cleaned\n",
    "cleaned_words1=[scrub_words(w) for w in raw_words]\n",
    "cleaned_stemmed_words1=[porter_stemmer.stem(word=word) for word in cleaned_words1]\n",
    "stemdf= pd.DataFrame({'raw_word': raw_words,'cleaned_word':cleaned_words1,'stemmed_word': cleaned_stemmed_words1})\n",
    "stemdf=stemdf[['raw_word','cleaned_word','stemmed_word']]\n",
    "stemdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "legislative-discount",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_word</th>\n",
       "      <th>cleaned_word</th>\n",
       "      <th>stemmed_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\rCustomer Service Supervisor/Tier - Isabella ...</td>\n",
       "      <td>Customer Service Supervisor Tier   Isabella Ca...</td>\n",
       "      <td>customer service supervisor tier   isabella ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\rEngineer / Scientist - IBM Microelectronics ...</td>\n",
       "      <td>Engineer   Scientist   IBM Microelectronics Di...</td>\n",
       "      <td>engineer   scientist   ibm microelectronics di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\rLTS Software Engineer Computational Lithogra...</td>\n",
       "      <td>LTS Software Engineer Computational Lithograph...</td>\n",
       "      <td>lts software engineer computational lithograph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TUTOR\\rWilliston VT - Email me on Indeed: ind...</td>\n",
       "      <td>TUTOR Williston VT   Email me on Indeed  indee...</td>\n",
       "      <td>tutor williston vt   email me on indeed  indee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\rIndependent Consultant - Self-employed\\rBurl...</td>\n",
       "      <td>Independent Consultant   Self employed Burling...</td>\n",
       "      <td>independent consultant   self employed burling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>\\rBrattleboro VT - Email me on Indeed: indeed....</td>\n",
       "      <td>Brattleboro VT   Email me on Indeed  indeed co...</td>\n",
       "      <td>brattleboro vt   email me on indeed  indeed co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>\\rResearch and Teaching Assistant - University...</td>\n",
       "      <td>Research and Teaching Assistant   University o...</td>\n",
       "      <td>research and teaching assistant   university o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>\\rMedical Coder - Highly Skilled - Entry Level...</td>\n",
       "      <td>Medical Coder   Highly Skilled   Entry Level S...</td>\n",
       "      <td>medical coder   highly skilled   entry level s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>\\rWaterbury VT - Email me on Indeed: indeed.co...</td>\n",
       "      <td>Waterbury VT   Email me on Indeed  indeed com ...</td>\n",
       "      <td>waterbury vt   email me on indeed  indeed com ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>\\rResearch and Development Scientist - Burling...</td>\n",
       "      <td>Research and Development Scientist   Burlingto...</td>\n",
       "      <td>research and development scientist   burlingto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              raw_word  \\\n",
       "0    \\rCustomer Service Supervisor/Tier - Isabella ...   \n",
       "1    \\rEngineer / Scientist - IBM Microelectronics ...   \n",
       "2    \\rLTS Software Engineer Computational Lithogra...   \n",
       "3     TUTOR\\rWilliston VT - Email me on Indeed: ind...   \n",
       "4    \\rIndependent Consultant - Self-employed\\rBurl...   \n",
       "..                                                 ...   \n",
       "120  \\rBrattleboro VT - Email me on Indeed: indeed....   \n",
       "121  \\rResearch and Teaching Assistant - University...   \n",
       "122  \\rMedical Coder - Highly Skilled - Entry Level...   \n",
       "123  \\rWaterbury VT - Email me on Indeed: indeed.co...   \n",
       "124  \\rResearch and Development Scientist - Burling...   \n",
       "\n",
       "                                          cleaned_word  \\\n",
       "0    Customer Service Supervisor Tier   Isabella Ca...   \n",
       "1    Engineer   Scientist   IBM Microelectronics Di...   \n",
       "2    LTS Software Engineer Computational Lithograph...   \n",
       "3    TUTOR Williston VT   Email me on Indeed  indee...   \n",
       "4    Independent Consultant   Self employed Burling...   \n",
       "..                                                 ...   \n",
       "120  Brattleboro VT   Email me on Indeed  indeed co...   \n",
       "121  Research and Teaching Assistant   University o...   \n",
       "122  Medical Coder   Highly Skilled   Entry Level S...   \n",
       "123  Waterbury VT   Email me on Indeed  indeed com ...   \n",
       "124  Research and Development Scientist   Burlingto...   \n",
       "\n",
       "                                          stemmed_word  \n",
       "0    customer service supervisor tier   isabella ca...  \n",
       "1    engineer   scientist   ibm microelectronics di...  \n",
       "2    lts software engineer computational lithograph...  \n",
       "3    tutor williston vt   email me on indeed  indee...  \n",
       "4    independent consultant   self employed burling...  \n",
       "..                                                 ...  \n",
       "120  brattleboro vt   email me on indeed  indeed co...  \n",
       "121  research and teaching assistant   university o...  \n",
       "122  medical coder   highly skilled   entry level s...  \n",
       "123  waterbury vt   email me on indeed  indeed com ...  \n",
       "124  research and development scientist   burlingto...  \n",
       "\n",
       "[125 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stem words already cleaned\n",
    "cleaned_words2=[scrub_words(w) for w in raw_words2]\n",
    "cleaned_stemmed_words2=[porter_stemmer.stem(word=word) for word in cleaned_words2]\n",
    "stemdf= pd.DataFrame({'raw_word': raw_words2,'cleaned_word':cleaned_words2,'stemmed_word': cleaned_stemmed_words2})\n",
    "stemdf=stemdf[['raw_word','cleaned_word','stemmed_word']]\n",
    "stemdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-burden",
   "metadata": {},
   "source": [
    "The resume dataset benefits greatly by using the scrub words function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "mobile-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Sheet_1.csv', encoding='latin-1')\n",
    "df.drop([\"Unnamed: 3\",\"Unnamed: 4\",\"Unnamed: 5\",\"Unnamed: 6\",\"Unnamed: 7\"], axis = 1, inplace =True)\n",
    "df = df.rename(columns={\"class\":\"Sentiment\", \"response_text\":\"Lables\"})\n",
    "df.dropna(axis=0, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "gothic-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Sheet_2.csv', encoding='latin-1')\n",
    "df2 = df2.drop([\"resume_id\"], axis = 1)\n",
    "df2 = df2.rename(columns={\"class\":\"Sentiment\", \"resume_text\":\"Lables\"})\n",
    "df2.dropna(axis=0, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "answering-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined  = pd.concat([df,df2])\n",
    "combined.replace(('flagged','not_flagged'),(1,0),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "environmental-houston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_id</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Lables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>response_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I try and avoid this sort of conflict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>response_2</td>\n",
       "      <td>1</td>\n",
       "      <td>Had a friend open up to me about his mental ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>response_3</td>\n",
       "      <td>1</td>\n",
       "      <td>I saved a girl from suicide once. She was goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>response_4</td>\n",
       "      <td>0</td>\n",
       "      <td>i cant think of one really...i think i may hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>response_5</td>\n",
       "      <td>0</td>\n",
       "      <td>Only really one friend who doesn't fit into th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  response_id  Sentiment                                             Lables\n",
       "0  response_1          0              I try and avoid this sort of conflict\n",
       "1  response_2          1  Had a friend open up to me about his mental ad...\n",
       "2  response_3          1  I saved a girl from suicide once. She was goin...\n",
       "3  response_4          0  i cant think of one really...i think i may hav...\n",
       "4  response_5          0  Only really one friend who doesn't fit into th..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-extreme",
   "metadata": {},
   "source": [
    "Both of the datasets are combined with two new columns: 'Labels' and 'Sentiment'. Sentiment is the same as the binary values assigned to the 'class' column earlier. 1 is 'flagged', 0 is 'not flagged'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-archives",
   "metadata": {},
   "source": [
    "## Tensorflow Deeplearning<a id='Tensorflow Deeplearning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-cancer",
   "metadata": {},
   "source": [
    "Deep learning is a category of machine learning models that use multi-layer neural networks. Deep learning is a technique for implementing machine learning. It uses neural networks to learn, sometimes, using decision trees may also be referred to as deep learning, but for the most part deep learning involves the use of neural networks. A neural network is a collection of layers that transform the input in some way to produce an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "strong-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences=combined['Lables'].tolist()\n",
    "labels=combined['Sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "brazilian-priority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "quality-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the sentences and labels into training and test sets\n",
    "training_size = int(len(sentences) * 0.8)\n",
    "\n",
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]\n",
    "\n",
    "# Make labels into numpy arrays for use with the network later\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "premium-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 300\n",
    "embedding_dim = 16\n",
    "max_length = 25\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(5, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "forbidden-mounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "6/6 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.6841\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6833 - accuracy: 0.7317\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.6762 - accuracy: 0.7212\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.6699 - accuracy: 0.6988\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.6608 - accuracy: 0.7013\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.6481 - accuracy: 0.7280\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.6436 - accuracy: 0.7100\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.6355 - accuracy: 0.7079\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.6329 - accuracy: 0.6981\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.6190 - accuracy: 0.7152\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.6060 - accuracy: 0.7294\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.6048 - accuracy: 0.7148\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.6136 - accuracy: 0.6831\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.5826 - accuracy: 0.7370\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.5828 - accuracy: 0.7171\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_padded, training_labels_final, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-error",
   "metadata": {},
   "source": [
    "It's worth mentioning that neural networks are great at fitting. Did this model overfit? Ideally, the model would generalize and learn patterns and attributes of the original text to determine a flag or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "honest-channels",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5856 - accuracy: 0.7317\n",
      "0.585637629032135 0.7317073345184326\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(testing_padded, testing_labels_final)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-patient",
   "metadata": {},
   "source": [
    "The out-of-sample validation has a slightly higher loss than the last epoch the model ran through. The accuracy is also slightly higher. A high delta in either category implies that there is overfitting within the model. I chose a train/test split of 80/20 because it gave me both in-sample and out-of-sample scores that were similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-indie",
   "metadata": {},
   "source": [
    "## Summary<a id='Summary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-victim",
   "metadata": {},
   "source": [
    "The nature of these data (well written, abundant) meant that minimal preprocessing was required when building a predictive model. Multinomial Naive Bayes and Random Forest classifiers were used on both datasets that had been affected by sklearn's count vectorizer. A gaussian naive bayes classifier was used to predict on the chatbot response data. Tensorflow deep learning was utilized on a combined dataset of both the chatbot responses and resumes. This TF deep learning model uses neural networks to predict, similar to an ensemble method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
